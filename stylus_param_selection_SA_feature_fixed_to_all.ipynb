{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from random import random\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, f1_score, precision_score, recall_score, log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import mplcursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_params(param_dict, curr_params=None):\n",
    "    \"\"\"\n",
    "    Function to choose parameters for next iteration\n",
    "    Inputs:\n",
    "    param_dict - Ordered dictionary of hyperparameter search space\n",
    "    curr_params - Dict of current hyperparameters\n",
    "    Output:\n",
    "    Dictionary of parameters\n",
    "    \"\"\"\n",
    "    if curr_params:\n",
    "        next_params = curr_params.copy()\n",
    "        param_to_update = np.random.choice(list(param_dict.keys()))\n",
    "        param_vals = param_dict[param_to_update]\n",
    "        curr_index = param_vals.index(curr_params[param_to_update])\n",
    "        if curr_index == 0:\n",
    "            next_params[param_to_update] = param_vals[1]\n",
    "        elif curr_index == len(param_vals) - 1:\n",
    "            next_params[param_to_update] = param_vals[curr_index - 1]\n",
    "        else:\n",
    "            next_params[param_to_update] = \\\n",
    "                param_vals[curr_index + np.random.choice([-1, 1])]\n",
    "    else:\n",
    "        next_params = dict()\n",
    "        for k, v in param_dict.items():\n",
    "            next_params[k] = np.random.choice(v)\n",
    "\n",
    "    return next_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from random import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_PATH = 'data/'\n",
    "OUTPUT_PATH = 'log/'\n",
    "\n",
    "# prepare data - stylus: train / val / test\n",
    "feature_df = pd.read_csv(DATA_PATH + 'feature_df_231227.csv')\n",
    "feature_df.dropna(axis=0,inplace=True)\n",
    "train_label_set = pd.read_csv(DATA_PATH + 'train_label_set.csv', usecols=['student_id', 'is_PHQ-9'])\n",
    "validation_label_set = pd.read_csv(DATA_PATH + 'test_label_set.csv', usecols=['student_id', 'is_PHQ-9'])\n",
    "\n",
    "train_set = pd.merge(feature_df, train_label_set, on='student_id', how='inner')\n",
    "train_set.drop(['student_id','quiz_id','week_id','try_id','device_os'], axis=1, inplace=True)\n",
    "X = train_set.copy()\n",
    "X.drop(['is_PHQ-9'], axis=1, inplace=True)\n",
    "y = train_set['is_PHQ-9']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "test_set = pd.merge(feature_df, validation_label_set, on='student_id', how='inner')\n",
    "test_set.drop(['student_id','quiz_id','week_id','try_id','device_os'], axis=1, inplace=True)\n",
    "X_test = test_set.copy()\n",
    "X_test.drop(['is_PHQ-9'], axis=1, inplace=True)\n",
    "y_test = test_set['is_PHQ-9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_PHQ-9\n",
       "0    23279\n",
       "1     3049\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_PHQ-9\n",
       "0    5790\n",
       "1     792\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_PHQ-9\n",
       "0    7343\n",
       "1     784\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_param = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'is_unbalance': True,\n",
    "    'verbose': -1, \n",
    "    'random_state': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter search space\n",
    "param_dict = OrderedDict()\n",
    "param_dict['learning_rate'] = [0.001, 0.003, 0.005, 0.01]\n",
    "param_dict['num_leaves'] = [31, 63, 127, 255, 511]\n",
    "param_dict['min_data_in_leaf'] = [5, 10, 20, 40]\n",
    "param_dict['max_bin'] = [255, 511, 1023]\n",
    "# param_dict['num_boost_round'] = [500, 1000, 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train model\n",
    "def train_model(curr_params, param, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train the model with given set of hyperparameters\n",
    "    curr_params - Dict of hyperparameters and chosen values\n",
    "    param - Dict of hyperparameters that are kept constant\n",
    "    Xtrain - Train Data\n",
    "    Xvalid - Validation Data\n",
    "    Ytrain - Train labels\n",
    "    Yvalid - Validaion labels\n",
    "    metric - Metric to compute model performance on\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    params_copy = param.copy()\n",
    "    params_copy.update(curr_params)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    \n",
    "    lr_to_num_boost_round = {\n",
    "    0.001: 10000,\n",
    "    0.003: 3000,\n",
    "    0.005: 2000,\n",
    "    0.01: 1000\n",
    "    }   \n",
    "\n",
    "    num_boost_round = lr_to_num_boost_round[params_copy['learning_rate']]\n",
    "    \n",
    "    lgb_params = {\n",
    "    'objective': 'binary', # fixed\n",
    "    'metric': 'binary_logloss', # 'auc' 'binary_error' \n",
    "    'boosting_type': 'gbdt', # 'dart' 'goss'\n",
    "    'learning_rate': params_copy['learning_rate'], # 0.01 ~ 0.3\n",
    "    'num_leaves': params_copy['num_leaves'], # 64 128 256 512 1024 2048\n",
    "    'max_depth': -1, # +1 -1~8\n",
    "    'min_data_in_leaf': params_copy['min_data_in_leaf'], # 20 ~ 900\n",
    "    'is_unbalance': True, # 'scale_pos_weight'\n",
    "    'max_bin': params_copy['max_bin'], # +100 or + 200 255 ~ 1024 \n",
    "    'verbose': -1, \n",
    "    'random_state': 1 # 0 or 1\n",
    "    }\n",
    "    \n",
    "    for train_index, val_index in tqdm(kf.split(X_train), desc=f'PHQ-9 - {len(X_train)} samples: '):\n",
    "        X_train_inner, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_inner, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        lgb_train = lgb.Dataset(X_train_inner, y_train_inner)\n",
    "        lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    " \n",
    "        gbm = lgb.train(lgb_params, lgb_train, num_boost_round=num_boost_round, valid_sets=lgb_eval)\n",
    "\n",
    "        models.append(gbm)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # model.fit(Xtrain, Ytrain)\n",
    "    # preds = model.predict(Xvalid)\n",
    "    # metric_val = metric(Yvalid, preds)\n",
    "    return models, num_boost_round\n",
    "    # return model, metric_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used in param selection, but used in feature selection\n",
    "# to get metric value for param selection, use 'get_metric_for_param_search'\n",
    "\n",
    "def get_metric(X_val, y_val, X_test, y_test, lgbm_models, object='acc'):\n",
    "    inner_threshold = 0.5 # for prob\n",
    "    threshold = 1         # for vote\n",
    "    \n",
    "    ############################################################ to derive metric value in test set\n",
    "    \n",
    "    y_val_true_total = y_val\n",
    "    y_val_pred_proba_total = np.zeros((len(y_val), 5))\n",
    "    \n",
    "    for i, model in enumerate(lgbm_models):\n",
    "        y_val_pred_proba = model.predict(X_val)\n",
    "\n",
    "        y_val_pred_proba_total[:, i] = y_val_pred_proba\n",
    "                                            \n",
    "    y_val_pred_total = np.where(y_val_pred_proba_total > inner_threshold, 1, 0)\n",
    "    y_val_final_proba = y_val_pred_proba_total.mean(axis=1)\n",
    "    y_val_final_pred = y_val_pred_total.sum(axis=1)\n",
    "    y_val_group_pred = np.where(y_val_final_pred > threshold, 1, 0)\n",
    "    \n",
    "    acc_val = accuracy_score(y_val_true_total, y_val_group_pred)\n",
    "    pre_val = precision_score(y_val_true_total, y_val_group_pred)\n",
    "    rec_val = recall_score(y_val_true_total, y_val_group_pred)\n",
    "    # loss_val = log_loss(y_val_true_total, y_val_final_proba)\n",
    "    f1_val = f1_score(y_val_true_total, y_val_group_pred)\n",
    "    auc_val = roc_auc_score(y_val_true_total, y_val_final_proba)\n",
    "    \n",
    "    ############################################################ to derive metric value in validation set\n",
    "\n",
    "    y_test_true_total = y_test\n",
    "    y_test_pred_proba_total = np.zeros((len(y_test), 5))\n",
    "    \n",
    "    for i, model in enumerate(lgbm_models):\n",
    "        y_test_pred_proba = model.predict(X_test)\n",
    "\n",
    "        y_test_pred_proba_total[:, i] = y_test_pred_proba\n",
    "                                            \n",
    "    y_test_pred_total = np.where(y_test_pred_proba_total > inner_threshold, 1, 0)\n",
    "    y_test_final_proba = y_test_pred_proba_total.mean(axis=1)\n",
    "    y_test_final_pred = y_test_pred_total.sum(axis=1)\n",
    "    y_test_group_pred = np.where(y_test_final_pred > threshold, 1, 0)\n",
    "    \n",
    "    acc_test = accuracy_score(y_test_true_total, y_test_group_pred)\n",
    "    pre_test = precision_score(y_test_true_total, y_test_group_pred)\n",
    "    rec_test = recall_score(y_test_true_total, y_test_group_pred)\n",
    "    # loss_test = log_loss(y_test_true_total, y_test_final_proba)\n",
    "    f1_test = f1_score(y_test_true_total, y_test_group_pred)\n",
    "    auc_test = roc_auc_score(y_test_true_total, y_test_group_pred)\n",
    "    \n",
    "    ############################################################\n",
    "    \n",
    "    if object == 'acc':\n",
    "        return acc_val,acc_test,pre_test,rec_test,f1_test,auc_test,inner_threshold,threshold \n",
    "    elif object == 'pre':\n",
    "        return pre_val,acc_test,pre_test,rec_test,f1_test,auc_test,inner_threshold,threshold \n",
    "    elif object == 'rec':\n",
    "        return rec_val,acc_test,pre_test,rec_test,f1_test,auc_test,inner_threshold,threshold \n",
    "    elif object == 'f1':\n",
    "        return f1_val,acc_test,pre_test,rec_test,f1_test,auc_test,inner_threshold,threshold \n",
    "    elif object == 'auc':\n",
    "        return auc_val,acc_test,pre_test,rec_test,f1_test,auc_test,inner_threshold,threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record each case (parameter set / acc / metric)\n",
    "\n",
    "def record_on_csv(acc_test,pre_test,rec_test,f1_test,auc_test,params,file_gen_time,inner_threshold,voting_threshold,num_boost_round): # file name format: 'param_select_{file_gen_time}.csv'\n",
    "    path = 'log/param_selection'\n",
    "    file_path = f'{path}/param_select_SA_{file_gen_time}.csv'\n",
    "\n",
    "    row_to_add = [acc_test,pre_test,rec_test,f1_test,auc_test,\n",
    "                  params['learning_rate'],params['num_leaves'],params['min_data_in_leaf'],params['max_bin'],\n",
    "                  inner_threshold,voting_threshold,num_boost_round]\n",
    "\n",
    "    # Check if file exists\n",
    "    file_exists = os.path.exists(file_path)\n",
    "\n",
    "    # Open the file in append mode if it exists, or write mode if it doesn't\n",
    "    mode = 'a' if file_exists else 'w'\n",
    "\n",
    "    with open(file_path, mode, newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # If the file is being created, you might want to write headers here\n",
    "        if not file_exists:\n",
    "            headers = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC', \n",
    "                       'objective', 'metric', 'boosting_type', 'learning_rate', 'num_leaves', 'max_depth', \n",
    "                       'min_data_in_leaf', 'is_unbalance', 'max_bin', 'verbose', 'random_state', \n",
    "                       'num_boost_round', 'inner_threshold', 'threshold']  # Replace with your headers\n",
    "            writer.writerow(headers)\n",
    "\n",
    "        # Write the row\n",
    "        writer.writerow(row_to_add)\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_from_csv(file_path):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure the index starts from 1\n",
    "    data.index = data.index + 1\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data['Accuracy'], label='Accuracy')\n",
    "    plt.plot(data['Precision'], label='Precision')\n",
    "    plt.plot(data['Recall'], label='Recall')\n",
    "    plt.plot(data['F1 Score'], label='F1 Score')\n",
    "    plt.plot(data['AUC'], label='AUC')\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Performance Metrics over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Adding cursors\n",
    "    cursor = mplcursors.cursor(hover=True)\n",
    "    @cursor.connect(\"add\")\n",
    "    def on_add(sel):\n",
    "        x, y = sel.target\n",
    "        epoch = int(x)\n",
    "        accuracy = data.at[epoch, 'Accuracy']\n",
    "        precision = data.at[epoch, 'Precision']\n",
    "        recall = data.at[epoch, 'Recall']\n",
    "        f1_score = data.at[epoch, 'F1 Score']\n",
    "        auc = data.at[epoch, 'AUC']\n",
    "        sel.annotation.set(text=f'Epoch: {epoch}\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1_score}\\nAUC: {auc}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_annealing(param_dict,\n",
    "                       const_param,\n",
    "                       X_train,\n",
    "                       X_val,\n",
    "                       y_train,\n",
    "                       y_val,\n",
    "                       maxiters=1000,\n",
    "                       alpha=0.9,\n",
    "                       beta=1.3,\n",
    "                       T_0=0.40,\n",
    "                       update_iters=3):\n",
    "    \"\"\"\n",
    "    Function to perform hyperparameter search using simulated annealing\n",
    "    Inputs:\n",
    "    param_dict - Ordered dictionary of Hyperparameter search space\n",
    "    const_param - Static parameters of the model\n",
    "    Xtrain - Train Data\n",
    "    Xvalid - Validation Data\n",
    "    Ytrain - Train labels\n",
    "    Yvalid - Validaion labels\n",
    "    fn_train - Function to train the model\n",
    "        (Should return model and metric value as tuple, sample commented above)\n",
    "    maxiters - Number of iterations to perform the parameter search\n",
    "    alpha - factor to reduce temperature\n",
    "    beta - constant in probability estimate\n",
    "    T_0 - Initial temperature\n",
    "    update_iters - # of iterations required to update temperature\n",
    "    Output:\n",
    "    Dataframe of the parameters explored and corresponding model performance\n",
    "    \"\"\"\n",
    "    file_gen_time = dt.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    columns = [*param_dict.keys()] + ['Metric', 'Best Metric']\n",
    "    results = pd.DataFrame(index=range(maxiters), columns=columns)\n",
    "    best_metric = -1.\n",
    "    prev_metric = -1.\n",
    "    prev_params = None\n",
    "    best_params = dict()\n",
    "    weights = list(map(lambda x: 10**x, list(range(len(param_dict)))))\n",
    "    hash_values = set()\n",
    "    T = T_0\n",
    "\n",
    "    for i in range(maxiters):\n",
    "        print('Starting Iteration {}'.format(i))\n",
    "        while True:\n",
    "            curr_params = choose_params(param_dict, prev_params)\n",
    "            indices = [param_dict[k].index(v) for k, v in curr_params.items()]\n",
    "            hash_val = sum([i * j for (i, j) in zip(weights, indices)])\n",
    "            if hash_val in hash_values:\n",
    "                print('Combination revisited')\n",
    "            else:\n",
    "                hash_values.add(hash_val)\n",
    "                break\n",
    "\n",
    "        # model, metric = fn_train(curr_params, const_param, X_train,\n",
    "        #                          X_valid, Y_train, Y_valid)\n",
    "        lgbm_models, num_boost_round = train_model(curr_params, const_param, X_train, y_train)\n",
    "        metric,acc_test,pre_test,rec_test,f1_test,auc_test,inner_threshold,voting_threshold = get_metric(X_val, y_val, X_test, y_test, lgbm_models, 'acc')\n",
    "        \n",
    "        file_path = record_on_csv(acc_test,pre_test,rec_test,f1_test,auc_test,curr_params,file_gen_time,inner_threshold,voting_threshold,num_boost_round)\n",
    "        plot_from_csv(file_path)\n",
    "        \n",
    "        if metric > prev_metric:\n",
    "            print('Local Improvement in metric from {:8.4f} to {:8.4f} '\n",
    "                  .format(prev_metric, metric) + ' - parameters accepted')\n",
    "            prev_params = curr_params.copy()\n",
    "            prev_metric = metric\n",
    "\n",
    "            if metric > best_metric:\n",
    "                print('Global improvement in metric from {:8.4f} to {:8.4f} '\n",
    "                      .format(best_metric, metric) +\n",
    "                      ' - best parameters updated')\n",
    "                best_metric = metric\n",
    "                best_params = curr_params.copy()\n",
    "                # best_model = model\n",
    "        else:\n",
    "            rnd = np.random.uniform()\n",
    "            diff = metric - prev_metric\n",
    "            threshold = np.exp(beta * diff / T)\n",
    "            if rnd < threshold:\n",
    "                print('No Improvement but parameters accepted. Metric change' +\n",
    "                      ': {:8.4f} threshold: {:6.4f} random number: {:6.4f}'\n",
    "                      .format(diff, threshold, rnd))\n",
    "                prev_metric = metric\n",
    "                prev_params = curr_params\n",
    "            else:\n",
    "                print('No Improvement and parameters rejected. Metric change' +\n",
    "                      ': {:8.4f} threshold: {:6.4f} random number: {:6.4f}'\n",
    "                      .format(diff, threshold, rnd))\n",
    "\n",
    "        # results.loc[i, list(curr_params.keys())] = list(curr_params.values())\n",
    "        # results.loc[i, 'Metric'] = metric\n",
    "        # results.loc[i, 'Best Metric'] = best_metric\n",
    "        # results.loc[i, 'Accuracy'] = acc_test\n",
    "        # results.loc[i, 'Precision'] = pre_test\n",
    "        # results.loc[i, 'Recall'] = rec_test\n",
    "        # results.loc[i, 'F1'] = f1_test\n",
    "        # results.loc[i, 'AUC'] = auc_test\n",
    "        # results.loc[i, 'Inner Threshold'] = inner_threshold\n",
    "        # results.loc[i, 'Voting Threshold'] = voting_threshold\n",
    "\n",
    "        if i % update_iters == 0:\n",
    "            T = alpha * T\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PHQ-9 - 26328 samples: : 5it [06:17, 75.58s/it]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'feature_subset' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msimulate_annealing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mconst_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                       \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                       \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmaxiters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                       \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mT_0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mupdate_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 57\u001b[0m, in \u001b[0;36msimulate_annealing\u001b[1;34m(param_dict, const_param, X_train, X_val, y_train, y_val, maxiters, alpha, beta, T_0, update_iters)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# model, metric = fn_train(curr_params, const_param, X_train,\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#                          X_valid, Y_train, Y_valid)\u001b[39;00m\n\u001b[0;32m     56\u001b[0m lgbm_models, num_boost_round \u001b[38;5;241m=\u001b[39m train_model(curr_params, const_param, X_train, y_train)\n\u001b[1;32m---> 57\u001b[0m metric,acc_test,pre_test,rec_test,f1_test,auc_test,inner_threshold,voting_threshold \u001b[38;5;241m=\u001b[39m \u001b[43mget_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgbm_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43macc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m file_path \u001b[38;5;241m=\u001b[39m record_on_csv(acc_test,pre_test,rec_test,f1_test,auc_test,curr_params,file_gen_time,inner_threshold,voting_threshold,num_boost_round)\n\u001b[0;32m     60\u001b[0m plot_from_csv(file_path)\n",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m, in \u001b[0;36mget_metric\u001b[1;34m(X_val, y_val, X_test, y_test, lgbm_models, object)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_metric\u001b[39m(X_val, y_val, X_test, y_test, lgbm_models, \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X_val, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m----> 6\u001b[0m         feature_subset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mfeature_subset\u001b[49m)\n\u001b[0;32m      8\u001b[0m     inner_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;66;03m# for prob\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m         \u001b[38;5;66;03m# for vote\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'feature_subset' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "simulate_annealing(param_dict,\n",
    "                       const_param,\n",
    "                       X_train,\n",
    "                       X_val,\n",
    "                       y_train,\n",
    "                       y_val,\n",
    "                       maxiters=1000,\n",
    "                       alpha=0.9,\n",
    "                       beta=1.3,\n",
    "                       T_0=0.40,\n",
    "                       update_iters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
